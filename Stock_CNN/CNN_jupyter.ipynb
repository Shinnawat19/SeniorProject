{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from statistics import mode\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Activation, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET50 (unrealize)\n",
    "SET50 = ['ADVANC', 'AOT', 'BANPU', 'BBL', 'BCP', 'BDMS',\n",
    "        'BEM', 'BH', 'BJC', 'BTS', 'CENTEL', 'CPALL', \n",
    "         'CPF', 'CPN', 'DTAC', 'EGCO', 'GLOBAL', 'HMPRO', \n",
    "         'INTUCH', 'IRPC', 'KBANK', 'KCE', 'KKP', 'KTB', 'LH', 'MINT', 'PTT',\n",
    "        'PTTEP', 'ROBINS', 'SCB', 'SCC', \n",
    "        'TCAP', 'TISCO', 'TMB', 'TOP', 'TRUE', 'TU']\n",
    "\n",
    "# remove (low data) : TPIPP WHA SPRC SAWAD PSH MTLS IVL GPSC EA CBG BPP BEAUTY PTTGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stock_data(symbol, year_start, year_end):\n",
    "    stock_data = []\n",
    "    with open('./SET50/' + symbol + '.BK.csv', 'r') as csv_file:\n",
    "        file_data = csv.reader(csv_file, delimiter=',')\n",
    "        file_data = list(file_data)[1:]\n",
    "        temp_value = 0\n",
    "        for row in file_data:\n",
    "            if row[2] is '':\n",
    "                continue\n",
    "            elif int(row[0][0:4]) >= year_start and int(row[0][0:4]) <= year_end:\n",
    "                temp = (float(row[2]) + float(row[3]))/2.0\n",
    "                if temp_value != 0:\n",
    "                    unrealize = (temp - temp_value) / temp_value\n",
    "                else:\n",
    "                    unrealize = 0\n",
    "                unrealize = [\"{0:.4f}\".format(unrealize)]\n",
    "                stock_data.append(unrealize)\n",
    "                temp_value = temp\n",
    "                \n",
    "    return stock_data\n",
    "\n",
    "def mean_square_error(actual, predict):\n",
    "    length = len(actual)\n",
    "    sums_error = 0.0\n",
    "    for i, j in zip(actual, predict):\n",
    "        sums_error += (i - j)**2\n",
    "    \n",
    "    return sums_error / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNewXTest(x_test, predict):\n",
    "    for (index, element) in enumerate(x_test[0]):\n",
    "        for i in range(len(element)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            elif i < 29:\n",
    "                element[i - 1] = element[i]\n",
    "            else:\n",
    "                element[i - 1] = element[i]\n",
    "                element[i] = predict[index]\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_datas = [ load_stock_data(x, 2007, 2017) for x in SET50 ]\n",
    "\n",
    "min_count = len(stock_datas[0])\n",
    "\n",
    "for index, element in enumerate(stock_datas):\n",
    "    if len(element) < min_count:\n",
    "        min_count = len(element)\n",
    "\n",
    "min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(len(stock_datas), 30, 1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3 ,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    model.add(Dropout(0.25))   \n",
    "\n",
    "    model.add(Flatten())  \n",
    "    model.add(Dense(128))  \n",
    "    model.add(Activation('relu'))  \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(stock_datas)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_sgd = create_model()\n",
    "\n",
    "optimizer = SGD(lr = 0.01, decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "model_sgd.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_adam = create_model()\n",
    "\n",
    "optimizer = Adam(lr = 0.01, decay= 1e-6)\n",
    "\n",
    "model_adam.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounds = min_count - 30\n",
    "train_index = int(rounds * 0.8)\n",
    "validate_index = train_index + int(rounds * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610, 37, 30, 1) (201, 37, 30, 1) (1, 37, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "count = 0\n",
    "index = 0\n",
    "while index < rounds:\n",
    "    temp = [i[index : index + 30] for i in stock_datas]\n",
    "    data.append(temp)\n",
    "    index += 1\n",
    "    \n",
    "x = np.asarray(data)\n",
    "x_train = x[0: train_index].astype('float32')\n",
    "x_validate = x[train_index: validate_index].astype('float32')\n",
    "x_test = x[validate_index: validate_index + 1].astype('float32')\n",
    "\n",
    "print(x_train.shape, x_validate.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610, 37) (201, 37) (202, 37)\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "index = 0\n",
    "while index < rounds:\n",
    "    temp = [stock_datas[i][index + 30][0] for i in range(len(stock_datas))]\n",
    "    target.append(temp)\n",
    "    index += 1\n",
    "    \n",
    "y = np.asarray(target)\n",
    "y_train = y[0: train_index].astype('float32')\n",
    "y_validate = y[train_index: validate_index].astype('float32')\n",
    "y_test = y[validate_index: rounds].astype('float32')\n",
    "\n",
    "print(y_train.shape, y_validate.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1610 samples, validate on 201 samples\n",
      "Epoch 1/100\n",
      "1610/1610 [==============================] - 2s 968us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 2/100\n",
      "1610/1610 [==============================] - 1s 462us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 3/100\n",
      "1610/1610 [==============================] - 1s 465us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 4/100\n",
      "1610/1610 [==============================] - 1s 462us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 5/100\n",
      "1610/1610 [==============================] - 1s 465us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 6/100\n",
      "1610/1610 [==============================] - 1s 471us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 7/100\n",
      "1610/1610 [==============================] - 1s 465us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 8/100\n",
      "1610/1610 [==============================] - 1s 463us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 9/100\n",
      "1610/1610 [==============================] - 1s 469us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 10/100\n",
      "1610/1610 [==============================] - 1s 465us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 11/100\n",
      "1610/1610 [==============================] - 1s 467us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 12/100\n",
      "1610/1610 [==============================] - 1s 474us/step - loss: 0.0016 - acc: 0.0335 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 13/100\n",
      "1610/1610 [==============================] - 1s 470us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 14/100\n",
      "1610/1610 [==============================] - 1s 467us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 15/100\n",
      "1610/1610 [==============================] - 1s 452us/step - loss: 0.0016 - acc: 0.0186 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 16/100\n",
      "1610/1610 [==============================] - 1s 452us/step - loss: 0.0016 - acc: 0.0211 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 17/100\n",
      "1610/1610 [==============================] - 1s 468us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 18/100\n",
      "1610/1610 [==============================] - 1s 476us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 19/100\n",
      "1610/1610 [==============================] - 1s 482us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 20/100\n",
      "1610/1610 [==============================] - 1s 484us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 21/100\n",
      "1610/1610 [==============================] - 1s 469us/step - loss: 0.0016 - acc: 0.0323 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 22/100\n",
      "1610/1610 [==============================] - 1s 479us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 23/100\n",
      "1610/1610 [==============================] - 1s 483us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 24/100\n",
      "1610/1610 [==============================] - 1s 488us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 25/100\n",
      "1610/1610 [==============================] - 1s 487us/step - loss: 0.0016 - acc: 0.0217 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 26/100\n",
      "1610/1610 [==============================] - 1s 478us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 27/100\n",
      "1610/1610 [==============================] - 1s 488us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 28/100\n",
      "1610/1610 [==============================] - 1s 487us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 29/100\n",
      "1610/1610 [==============================] - 1s 491us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 30/100\n",
      "1610/1610 [==============================] - 1s 485us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 31/100\n",
      "1610/1610 [==============================] - 1s 486us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 32/100\n",
      "1610/1610 [==============================] - 1s 470us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 33/100\n",
      "1610/1610 [==============================] - 1s 474us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 34/100\n",
      "1610/1610 [==============================] - 1s 466us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 35/100\n",
      "1610/1610 [==============================] - 1s 462us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 36/100\n",
      "1610/1610 [==============================] - 1s 458us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 37/100\n",
      "1610/1610 [==============================] - 1s 469us/step - loss: 0.0016 - acc: 0.0323 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 38/100\n",
      "1610/1610 [==============================] - 1s 463us/step - loss: 0.0016 - acc: 0.0242 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 39/100\n",
      "1610/1610 [==============================] - 1s 476us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 40/100\n",
      "1610/1610 [==============================] - 1s 480us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 41/100\n",
      "1610/1610 [==============================] - 1s 477us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 42/100\n",
      "1610/1610 [==============================] - 1s 494us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 43/100\n",
      "1610/1610 [==============================] - 1s 494us/step - loss: 0.0016 - acc: 0.0211 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 44/100\n",
      "1610/1610 [==============================] - 1s 481us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 45/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 46/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 47/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 48/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 49/100\n",
      "1610/1610 [==============================] - 0s 307us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 50/100\n",
      "1610/1610 [==============================] - 1s 343us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0299\n",
      "Epoch 51/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 52/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 53/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 54/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 55/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 56/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 57/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 0s 306us/step - loss: 0.0016 - acc: 0.0186 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 59/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 60/100\n",
      "1610/1610 [==============================] - 1s 325us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 61/100\n",
      "1610/1610 [==============================] - 1s 361us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 62/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0016 - acc: 0.0410 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 63/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 64/100\n",
      "1610/1610 [==============================] - 0s 303us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 65/100\n",
      "1610/1610 [==============================] - 0s 306us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 66/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0335 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 67/100\n",
      "1610/1610 [==============================] - 0s 296us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 68/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0224 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 69/100\n",
      "1610/1610 [==============================] - 0s 298us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 70/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 71/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0199 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 72/100\n",
      "1610/1610 [==============================] - 0s 302us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 73/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 74/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0348\n",
      "Epoch 75/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0211 - val_loss: 9.2411e-04 - val_acc: 0.0398\n",
      "Epoch 76/100\n",
      "1610/1610 [==============================] - 0s 304us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2411e-04 - val_acc: 0.0398\n",
      "Epoch 77/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0398\n",
      "Epoch 78/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2411e-04 - val_acc: 0.0398\n",
      "Epoch 79/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2411e-04 - val_acc: 0.0398\n",
      "Epoch 80/100\n",
      "1610/1610 [==============================] - 0s 306us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 81/100\n",
      "1610/1610 [==============================] - 0s 305us/step - loss: 0.0016 - acc: 0.0242 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 82/100\n",
      "1610/1610 [==============================] - 0s 307us/step - loss: 0.0016 - acc: 0.0242 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 83/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 84/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 85/100\n",
      "1610/1610 [==============================] - 0s 307us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 86/100\n",
      "1610/1610 [==============================] - 0s 302us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 87/100\n",
      "1610/1610 [==============================] - 0s 303us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 88/100\n",
      "1610/1610 [==============================] - 0s 300us/step - loss: 0.0016 - acc: 0.0180 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 89/100\n",
      "1610/1610 [==============================] - 0s 302us/step - loss: 0.0016 - acc: 0.0360 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 90/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 91/100\n",
      "1610/1610 [==============================] - 0s 299us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 92/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 93/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0329 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 94/100\n",
      "1610/1610 [==============================] - 0s 303us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 95/100\n",
      "1610/1610 [==============================] - 0s 306us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 96/100\n",
      "1610/1610 [==============================] - 0s 301us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 97/100\n",
      "1610/1610 [==============================] - 1s 314us/step - loss: 0.0016 - acc: 0.0217 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 98/100\n",
      "1610/1610 [==============================] - 0s 303us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 99/100\n",
      "1610/1610 [==============================] - 0s 300us/step - loss: 0.0016 - acc: 0.0242 - val_loss: 9.2411e-04 - val_acc: 0.0448\n",
      "Epoch 100/100\n",
      "1610/1610 [==============================] - 0s 298us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2411e-04 - val_acc: 0.0448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2163c8603c8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "model_sgd.fit(x_train, y_train, validation_data = (x_validate, y_validate), epochs = epochs, shuffle = True, batch_size = 100, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1610 samples, validate on 201 samples\n",
      "Epoch 1/100\n",
      "1610/1610 [==============================] - 1s 629us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2469e-04 - val_acc: 0.0100\n",
      "Epoch 2/100\n",
      "1610/1610 [==============================] - 1s 331us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2444e-04 - val_acc: 0.0100\n",
      "Epoch 3/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2504e-04 - val_acc: 0.0100\n",
      "Epoch 4/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0435 - val_loss: 9.2459e-04 - val_acc: 0.0100\n",
      "Epoch 5/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0441 - val_loss: 9.2467e-04 - val_acc: 0.0100\n",
      "Epoch 6/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0015 - acc: 0.0466 - val_loss: 9.2481e-04 - val_acc: 0.0149\n",
      "Epoch 7/100\n",
      "1610/1610 [==============================] - 1s 321us/step - loss: 0.0016 - acc: 0.0354 - val_loss: 9.2435e-04 - val_acc: 0.0100\n",
      "Epoch 8/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0015 - acc: 0.0398 - val_loss: 9.2685e-04 - val_acc: 0.0100\n",
      "Epoch 9/100\n",
      "1610/1610 [==============================] - 1s 329us/step - loss: 0.0016 - acc: 0.0540 - val_loss: 9.2431e-04 - val_acc: 0.0100\n",
      "Epoch 10/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2514e-04 - val_acc: 0.0100\n",
      "Epoch 11/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2493e-04 - val_acc: 0.0100\n",
      "Epoch 12/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0015 - acc: 0.0472 - val_loss: 9.2622e-04 - val_acc: 0.0100\n",
      "Epoch 13/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0404 - val_loss: 9.2443e-04 - val_acc: 0.0945\n",
      "Epoch 14/100\n",
      "1610/1610 [==============================] - 1s 326us/step - loss: 0.0016 - acc: 0.0590 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 15/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0015 - acc: 0.0528 - val_loss: 9.2474e-04 - val_acc: 0.0100\n",
      "Epoch 16/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0015 - acc: 0.0478 - val_loss: 9.2436e-04 - val_acc: 0.0100\n",
      "Epoch 17/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0015 - acc: 0.0509 - val_loss: 9.2431e-04 - val_acc: 0.0100\n",
      "Epoch 18/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0528 - val_loss: 9.2557e-04 - val_acc: 0.0100\n",
      "Epoch 19/100\n",
      "1610/1610 [==============================] - 1s 329us/step - loss: 0.0016 - acc: 0.0422 - val_loss: 9.2442e-04 - val_acc: 0.0100\n",
      "Epoch 20/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0398 - val_loss: 9.2648e-04 - val_acc: 0.0100\n",
      "Epoch 21/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.3173e-04 - val_acc: 0.0299\n",
      "Epoch 22/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2482e-04 - val_acc: 0.0100\n",
      "Epoch 23/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0484 - val_loss: 9.2518e-04 - val_acc: 0.0100\n",
      "Epoch 24/100\n",
      "1610/1610 [==============================] - 1s 317us/step - loss: 0.0016 - acc: 0.0379 - val_loss: 9.2472e-04 - val_acc: 0.0100\n",
      "Epoch 25/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2464e-04 - val_acc: 0.0100\n",
      "Epoch 26/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0435 - val_loss: 9.2472e-04 - val_acc: 0.0100\n",
      "Epoch 27/100\n",
      "1610/1610 [==============================] - 1s 314us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2464e-04 - val_acc: 0.0100\n",
      "Epoch 28/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2479e-04 - val_acc: 0.0100\n",
      "Epoch 29/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2492e-04 - val_acc: 0.0100\n",
      "Epoch 30/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2499e-04 - val_acc: 0.0100\n",
      "Epoch 31/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2571e-04 - val_acc: 0.0100\n",
      "Epoch 32/100\n",
      "1610/1610 [==============================] - 1s 333us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2619e-04 - val_acc: 0.0100\n",
      "Epoch 33/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2591e-04 - val_acc: 0.0100\n",
      "Epoch 34/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0015 - acc: 0.0453 - val_loss: 9.2579e-04 - val_acc: 0.0100\n",
      "Epoch 35/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0016 - acc: 0.0497 - val_loss: 9.2575e-04 - val_acc: 0.0100\n",
      "Epoch 36/100\n",
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0435 - val_loss: 9.2520e-04 - val_acc: 0.0100\n",
      "Epoch 37/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0435 - val_loss: 9.2450e-04 - val_acc: 0.0100\n",
      "Epoch 38/100\n",
      "1610/1610 [==============================] - 0s 307us/step - loss: 0.0016 - acc: 0.0503 - val_loss: 9.2499e-04 - val_acc: 0.0100\n",
      "Epoch 39/100\n",
      "1610/1610 [==============================] - 1s 319us/step - loss: 0.0015 - acc: 0.0441 - val_loss: 9.2534e-04 - val_acc: 0.0100\n",
      "Epoch 40/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0441 - val_loss: 9.2539e-04 - val_acc: 0.0100\n",
      "Epoch 41/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2524e-04 - val_acc: 0.0100\n",
      "Epoch 42/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0422 - val_loss: 9.2510e-04 - val_acc: 0.0100\n",
      "Epoch 43/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2490e-04 - val_acc: 0.0100\n",
      "Epoch 44/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2503e-04 - val_acc: 0.0100\n",
      "Epoch 45/100\n",
      "1610/1610 [==============================] - 1s 318us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2530e-04 - val_acc: 0.0100\n",
      "Epoch 46/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2512e-04 - val_acc: 0.0100\n",
      "Epoch 47/100\n",
      "1610/1610 [==============================] - 0s 307us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2507e-04 - val_acc: 0.0100\n",
      "Epoch 48/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2559e-04 - val_acc: 0.0100\n",
      "Epoch 49/100\n",
      "1610/1610 [==============================] - 1s 317us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2547e-04 - val_acc: 0.0100\n",
      "Epoch 50/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2539e-04 - val_acc: 0.0100\n",
      "Epoch 51/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2527e-04 - val_acc: 0.0100\n",
      "Epoch 52/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2518e-04 - val_acc: 0.0100\n",
      "Epoch 53/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2510e-04 - val_acc: 0.0100\n",
      "Epoch 54/100\n",
      "1610/1610 [==============================] - 0s 309us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2518e-04 - val_acc: 0.0100\n",
      "Epoch 55/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2564e-04 - val_acc: 0.0100\n",
      "Epoch 56/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2683e-04 - val_acc: 0.0100\n",
      "Epoch 57/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2675e-04 - val_acc: 0.0100\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610/1610 [==============================] - 0s 308us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2639e-04 - val_acc: 0.0100\n",
      "Epoch 59/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2610e-04 - val_acc: 0.0100\n",
      "Epoch 60/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2557e-04 - val_acc: 0.0100\n",
      "Epoch 61/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2512e-04 - val_acc: 0.0100\n",
      "Epoch 62/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0016 - acc: 0.0429 - val_loss: 9.2501e-04 - val_acc: 0.0100\n",
      "Epoch 63/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2513e-04 - val_acc: 0.0100\n",
      "Epoch 64/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2524e-04 - val_acc: 0.0100\n",
      "Epoch 65/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2494e-04 - val_acc: 0.0100\n",
      "Epoch 66/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0016 - acc: 0.0441 - val_loss: 9.2508e-04 - val_acc: 0.0100\n",
      "Epoch 67/100\n",
      "1610/1610 [==============================] - 1s 319us/step - loss: 0.0016 - acc: 0.0435 - val_loss: 9.2483e-04 - val_acc: 0.0100\n",
      "Epoch 68/100\n",
      "1610/1610 [==============================] - 1s 319us/step - loss: 0.0015 - acc: 0.0435 - val_loss: 9.2508e-04 - val_acc: 0.0100\n",
      "Epoch 69/100\n",
      "1610/1610 [==============================] - 1s 312us/step - loss: 0.0016 - acc: 0.0460 - val_loss: 9.2496e-04 - val_acc: 0.0100\n",
      "Epoch 70/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0015 - acc: 0.0429 - val_loss: 9.2499e-04 - val_acc: 0.0100\n",
      "Epoch 71/100\n",
      "1610/1610 [==============================] - 1s 328us/step - loss: 0.0015 - acc: 0.0441 - val_loss: 9.2533e-04 - val_acc: 0.0100\n",
      "Epoch 72/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0015 - acc: 0.0429 - val_loss: 9.2529e-04 - val_acc: 0.0100\n",
      "Epoch 73/100\n",
      "1610/1610 [==============================] - 1s 317us/step - loss: 0.0015 - acc: 0.0460 - val_loss: 9.2512e-04 - val_acc: 0.0100\n",
      "Epoch 74/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0441 - val_loss: 9.2508e-04 - val_acc: 0.0100\n",
      "Epoch 75/100\n",
      "1610/1610 [==============================] - 1s 327us/step - loss: 0.0016 - acc: 0.0460 - val_loss: 9.2439e-04 - val_acc: 0.0299\n",
      "Epoch 76/100\n",
      "1610/1610 [==============================] - 1s 318us/step - loss: 0.0016 - acc: 0.0497 - val_loss: 9.2473e-04 - val_acc: 0.0100\n",
      "Epoch 77/100\n",
      "1610/1610 [==============================] - 1s 311us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2478e-04 - val_acc: 0.0100\n",
      "Epoch 78/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0015 - acc: 0.0435 - val_loss: 9.2555e-04 - val_acc: 0.0100\n",
      "Epoch 79/100\n",
      "1610/1610 [==============================] - 1s 324us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2511e-04 - val_acc: 0.0100\n",
      "Epoch 80/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0015 - acc: 0.0453 - val_loss: 9.2477e-04 - val_acc: 0.0100\n",
      "Epoch 81/100\n",
      "1610/1610 [==============================] - 1s 315us/step - loss: 0.0016 - acc: 0.0466 - val_loss: 9.2965e-04 - val_acc: 0.0149\n",
      "Epoch 82/100\n",
      "1610/1610 [==============================] - 1s 322us/step - loss: 0.0016 - acc: 0.0398 - val_loss: 9.2467e-04 - val_acc: 0.0100\n",
      "Epoch 83/100\n",
      "1610/1610 [==============================] - 1s 331us/step - loss: 0.0016 - acc: 0.0466 - val_loss: 9.2447e-04 - val_acc: 0.0100\n",
      "Epoch 84/100\n",
      "1610/1610 [==============================] - 1s 320us/step - loss: 0.0015 - acc: 0.0497 - val_loss: 9.2447e-04 - val_acc: 0.0100\n",
      "Epoch 85/100\n",
      "1610/1610 [==============================] - 1s 314us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2440e-04 - val_acc: 0.0149\n",
      "Epoch 86/100\n",
      "1610/1610 [==============================] - 1s 318us/step - loss: 0.0016 - acc: 0.0466 - val_loss: 9.2449e-04 - val_acc: 0.0100\n",
      "Epoch 87/100\n",
      "1610/1610 [==============================] - 1s 319us/step - loss: 0.0016 - acc: 0.0460 - val_loss: 9.2469e-04 - val_acc: 0.0100\n",
      "Epoch 88/100\n",
      "1610/1610 [==============================] - 1s 326us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2460e-04 - val_acc: 0.0100\n",
      "Epoch 89/100\n",
      "1610/1610 [==============================] - 1s 324us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2441e-04 - val_acc: 0.0100\n",
      "Epoch 90/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2462e-04 - val_acc: 0.0100\n",
      "Epoch 91/100\n",
      "1610/1610 [==============================] - 1s 317us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2505e-04 - val_acc: 0.0100\n",
      "Epoch 92/100\n",
      "1610/1610 [==============================] - 1s 319us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2505e-04 - val_acc: 0.0100\n",
      "Epoch 93/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2463e-04 - val_acc: 0.0100\n",
      "Epoch 94/100\n",
      "1610/1610 [==============================] - 1s 341us/step - loss: 0.0016 - acc: 0.0447 - val_loss: 9.2482e-04 - val_acc: 0.0100\n",
      "Epoch 95/100\n",
      "1610/1610 [==============================] - 1s 321us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2512e-04 - val_acc: 0.0100\n",
      "Epoch 96/100\n",
      "1610/1610 [==============================] - 1s 314us/step - loss: 0.0015 - acc: 0.0453 - val_loss: 9.2512e-04 - val_acc: 0.0100\n",
      "Epoch 97/100\n",
      "1610/1610 [==============================] - 1s 317us/step - loss: 0.0015 - acc: 0.0441 - val_loss: 9.2495e-04 - val_acc: 0.0100\n",
      "Epoch 98/100\n",
      "1610/1610 [==============================] - 1s 313us/step - loss: 0.0015 - acc: 0.0447 - val_loss: 9.2472e-04 - val_acc: 0.0100\n",
      "Epoch 99/100\n",
      "1610/1610 [==============================] - 0s 310us/step - loss: 0.0015 - acc: 0.0429 - val_loss: 9.2452e-04 - val_acc: 0.0100\n",
      "Epoch 100/100\n",
      "1610/1610 [==============================] - 1s 316us/step - loss: 0.0016 - acc: 0.0453 - val_loss: 9.2473e-04 - val_acc: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2163d4d7f98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adam.fit(x_train, y_train, validation_data = (x_validate, y_validate), epochs = epochs, shuffle = True, batch_size = 100, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sgd = x_test.copy()\n",
    "test_adam = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_sgd = []\n",
    "for (index, element) in enumerate(y_test):\n",
    "    predict = model_sgd.predict(test_sgd, verbose = 0)\n",
    "    error_sgd.append(mean_square_error(element, predict[0]))\n",
    "    test_sgd = findNewXTest(test_sgd, predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_adam = []\n",
    "for (index, element) in enumerate(y_test):\n",
    "    predict = model_adam.predict(test_adam, verbose = 0)\n",
    "    error_adam.append(mean_square_error(element, predict[0]))\n",
    "    test_adam = findNewXTest(test_adam, predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0 : 0.00066594174307 0.000667250877993 -1.30913492246e-06\n",
      "round 1 : 0.000818180652858 0.0008213196626 -3.13900974266e-06\n",
      "round 2 : 0.000705636720748 0.00071119942773 -5.56270698159e-06\n",
      "round 3 : 0.000816184234298 0.000824189024086 -8.00478978801e-06\n",
      "round 4 : 0.000697512968248 0.000700713118607 -3.20015035906e-06\n",
      "round 5 : 0.000945144139829 0.000950547277623 -5.4031377937e-06\n",
      "round 6 : 0.00100212401495 0.00100546167169 -3.33765673572e-06\n",
      "round 7 : 0.000912119549368 0.000916068131558 -3.94858219002e-06\n",
      "round 8 : 0.00128159532257 0.00127777195666 3.82336590764e-06\n",
      "round 9 : 0.000917146880238 0.000914036093869 3.1107863684e-06\n",
      "round 10 : 0.000772354172522 0.000773742445004 -1.38827248212e-06\n",
      "round 11 : 0.00135696530187 0.00135594242655 1.02287531766e-06\n",
      "round 12 : 0.00111079249442 0.00110912189663 1.6705977906e-06\n",
      "round 13 : 0.00131877277543 0.00132263601658 -3.86324114927e-06\n",
      "round 14 : 0.0010610469099 0.00106253963965 -1.49272975265e-06\n",
      "round 15 : 0.000744956608903 0.000746230572332 -1.27396342885e-06\n",
      "round 16 : 0.00116562580412 0.00116765588176 -2.03007764293e-06\n",
      "round 17 : 0.000567161208878 0.000570849843 -3.68863412214e-06\n",
      "round 18 : 0.000720268257666 0.000722242192464 -1.97393479827e-06\n",
      "round 19 : 0.000744501444399 0.000744490933131 1.05112682563e-08\n",
      "round 20 : 0.000739698504645 0.000739737571753 -3.90671074409e-08\n",
      "round 21 : 0.001023035613 0.00102646105924 -3.42544624312e-06\n",
      "round 22 : 0.000579524742579 0.000582205861804 -2.68111922457e-06\n",
      "round 23 : 0.00063939313426 0.000640239092385 -8.45958125813e-07\n",
      "round 24 : 0.000844374254832 0.000843066557284 1.30769754842e-06\n",
      "round 25 : 0.000692866583361 0.00069578782238 -2.92123901991e-06\n",
      "round 26 : 0.000707791766808 0.000708196969677 -4.05202869535e-07\n",
      "round 27 : 0.000575506540814 0.000580936855084 -5.43031426963e-06\n",
      "round 28 : 0.000803508288794 0.000809813479362 -6.30519056793e-06\n",
      "round 29 : 0.000783188964692 0.000784501156093 -1.31219140104e-06\n",
      "round 30 : 0.000843269017038 0.000847235404941 -3.9663879026e-06\n",
      "round 31 : 0.000903633298107 0.000908021733958 -4.38843585061e-06\n",
      "round 32 : 0.000626882843705 0.000627264310654 -3.81466948814e-07\n",
      "round 33 : 0.000784033954939 0.000783269048424 7.64906515301e-07\n",
      "round 34 : 0.000815116874152 0.000821694382845 -6.57750869315e-06\n",
      "round 35 : 0.000551138692132 0.00055944176322 -8.30307108782e-06\n",
      "round 36 : 0.000889928332321 0.000895482639812 -5.55430749118e-06\n",
      "round 37 : 0.000575576736442 0.000580590854028 -5.01411758574e-06\n",
      "round 38 : 0.000804856003027 0.000804990944653 -1.34941626832e-07\n",
      "round 39 : 0.00108001422 0.00108244686754 -2.43264753509e-06\n",
      "round 40 : 0.000977900372655 0.000977760886979 1.39485675951e-07\n",
      "round 41 : 0.000999294676246 0.000999755111671 -4.60435425627e-07\n",
      "round 42 : 0.00125839033597 0.00125498307244 3.40726353195e-06\n",
      "round 43 : 0.000985824694808 0.000992985024493 -7.16032968498e-06\n",
      "round 44 : 0.0010157632509 0.00101985179363 -4.08854273589e-06\n",
      "round 45 : 0.000870336417462 0.000871058158911 -7.21741448168e-07\n",
      "round 46 : 0.000738359886774 0.000733210820063 5.14906671102e-06\n",
      "round 47 : 0.000871092808605 0.000871433181262 -3.40372657035e-07\n",
      "round 48 : 0.000740373184089 0.000744381550007 -4.00836591856e-06\n",
      "round 49 : 0.000689132236888 0.000692832501594 -3.70026470605e-06\n",
      "round 50 : 0.000680469371817 0.000682464857735 -1.99548591884e-06\n",
      "round 51 : 0.000737610867 0.000738237064632 -6.26197632271e-07\n",
      "round 52 : 0.000835940106368 0.000834361357278 1.57874909059e-06\n",
      "round 53 : 0.00109061755531 0.00109032425938 2.93295933245e-07\n",
      "round 54 : 0.000981152681281 0.000986616962654 -5.46428137316e-06\n",
      "round 55 : 0.00128447099003 0.00128535673509 -8.85745057418e-07\n",
      "round 56 : 0.00113282215249 0.00112613519484 6.68695765204e-06\n",
      "round 57 : 0.00101520611024 0.00101621754828 -1.0114380348e-06\n",
      "round 58 : 0.00148124053608 0.00147982255947 1.41797661319e-06\n",
      "round 59 : 0.00112023653404 0.00112203125477 -1.79472072898e-06\n",
      "round 60 : 0.000922581079206 0.000922119349164 4.61730041308e-07\n",
      "round 61 : 0.000947692687106 0.000945204167633 2.48851947267e-06\n",
      "round 62 : 0.000819705110255 0.000815976334263 3.7287759917e-06\n",
      "round 63 : 0.000660154977863 0.000662727257768 -2.57227990491e-06\n",
      "round 64 : 0.00100728747022 0.0010084674672 -1.17999697453e-06\n",
      "round 65 : 0.00147988432261 0.0014822292421 -2.34491949284e-06\n",
      "round 66 : 0.000999295404815 0.00100018516552 -8.89760703181e-07\n",
      "round 67 : 0.000704517452452 0.000707966584736 -3.44913228347e-06\n",
      "round 68 : 0.00118855184078 0.00118873424694 -1.82406166357e-07\n",
      "round 69 : 0.000857288843143 0.000849201679195 8.08716394726e-06\n",
      "round 70 : 0.000666350714878 0.000666428687308 -7.79724304077e-08\n",
      "round 71 : 0.00106041994963 0.00105992757546 4.92374167075e-07\n",
      "round 72 : 0.000800291014475 0.000798107433969 2.18358050574e-06\n",
      "round 73 : 0.000651887530887 0.000652685348237 -7.97817349826e-07\n",
      "round 74 : 0.000685194121404 0.000691598401402 -6.40427999785e-06\n",
      "round 75 : 0.000664612661349 0.000671165011492 -6.55235014216e-06\n",
      "round 76 : 0.000573011141839 0.000578206588364 -5.19544652445e-06\n",
      "round 77 : 0.000869949144789 0.00086604431411 3.90483067922e-06\n",
      "round 78 : 0.000867339856387 0.000867880655336 -5.40798949477e-07\n",
      "round 79 : 0.000935083259891 0.000937379808917 -2.29654902614e-06\n",
      "round 80 : 0.00125244492203 0.00125020305091 2.24187112245e-06\n",
      "round 81 : 0.000873729991833 0.000877149351608 -3.4193597757e-06\n",
      "round 82 : 0.0012453245183 0.00125170189437 -6.37737606234e-06\n",
      "round 83 : 0.00116043176077 0.00115775069515 2.68106562113e-06\n",
      "round 84 : 0.000700776394661 0.000699997284881 7.79109779826e-07\n",
      "round 85 : 0.000640212742164 0.000640484987064 -2.72244900368e-07\n",
      "round 86 : 0.000807355413088 0.000804403471759 2.9519413283e-06\n",
      "round 87 : 0.000773104688511 0.000773553242941 -4.48554430373e-07\n",
      "round 88 : 0.000900330567698 0.00090878926599 -8.45869829201e-06\n",
      "round 89 : 0.00102984183874 0.00102976677609 7.50626480781e-08\n",
      "round 90 : 0.000851955358513 0.000850121351021 1.83400749264e-06\n",
      "round 91 : 0.000945737087802 0.000947170541566 -1.43345376354e-06\n",
      "round 92 : 0.000936008010633 0.000937380896322 -1.3728856885e-06\n",
      "round 93 : 0.000999330555753 0.000999069625355 2.609303981e-07\n",
      "round 94 : 0.00119006839143 0.00119012991673 -6.15252918211e-08\n",
      "round 95 : 0.00170554472124 0.00170075139027 4.79333096771e-06\n",
      "round 96 : 0.00160717863185 0.00160307944117 4.09919068364e-06\n",
      "round 97 : 0.00181528465651 0.00181141114064 3.87351586783e-06\n",
      "round 98 : 0.00175202132517 0.00174484061343 7.18071173699e-06\n",
      "round 99 : 0.00215309759986 0.00214841170753 4.68589233379e-06\n",
      "round 100 : 0.000968787508049 0.000977322093011 -8.53458496265e-06\n",
      "round 101 : 0.000664462626805 0.000672106603736 -7.64397693101e-06\n",
      "round 102 : 0.00157283270717 0.00157860533416 -5.77262699544e-06\n",
      "round 103 : 0.00075248035224 0.00075642532315 -3.94497091007e-06\n",
      "round 104 : 0.00135720543638 0.00135806398322 -8.58546837317e-07\n",
      "round 105 : 0.00132919653186 0.00132780014418 1.3963876855e-06\n",
      "round 106 : 0.00105342853694 0.00105106682469 2.36171225652e-06\n",
      "round 107 : 0.00217834132095 0.00217941208398 -1.0707630365e-06\n",
      "round 108 : 0.000758860896532 0.000758719549745 1.41346787083e-07\n",
      "round 109 : 0.000763905106646 0.000765991127889 -2.08602124356e-06\n",
      "round 110 : 0.0010507710491 0.00105343857851 -2.66752941809e-06\n",
      "round 111 : 0.00097169040623 0.000972513874585 -8.23468355548e-07\n",
      "round 112 : 0.00110571607977 0.00110835591343 -2.6398336519e-06\n",
      "round 113 : 0.000594444478004 0.000599165315912 -4.72083790799e-06\n",
      "round 114 : 0.000683784195314 0.000688575246145 -4.79105083125e-06\n",
      "round 115 : 0.00106699853492 0.00106927678221 -2.27824729346e-06\n",
      "round 116 : 0.00107887873264 0.00107870114743 1.77585216646e-07\n",
      "round 117 : 0.00109074098755 0.00108999044651 7.50541031905e-07\n",
      "round 118 : 0.000926081481819 0.000926364683415 -2.83201596121e-07\n",
      "round 119 : 0.000719866076052 0.000717810345975 2.05573007692e-06\n",
      "round 120 : 0.000906963775074 0.000908391303602 -1.42752852856e-06\n",
      "round 121 : 0.000652702636758 0.00065973161487 -7.02897811201e-06\n",
      "round 122 : 0.00061662713396 0.000617038206818 -4.11072857725e-07\n",
      "round 123 : 0.000623103757648 0.000621354748636 1.74900901176e-06\n",
      "round 124 : 0.000641963443233 0.000649079522278 -7.11607904583e-06\n",
      "round 125 : 0.000679636100299 0.000683753194649 -4.11709435012e-06\n",
      "round 126 : 0.000922618138319 0.000931074759877 -8.45662155727e-06\n",
      "round 127 : 0.0011877113341 0.00119353437506 -5.82304095395e-06\n",
      "round 128 : 0.0010316461014 0.00103509574752 -3.44964612124e-06\n",
      "round 129 : 0.00101064993481 0.00101030662221 3.43312598189e-07\n",
      "round 130 : 0.00096066372571 0.000965284440439 -4.62071472916e-06\n",
      "round 131 : 0.000712686197033 0.000722303612559 -9.6174155254e-06\n",
      "round 132 : 0.000722928108363 0.000730077476227 -7.14936786333e-06\n",
      "round 133 : 0.000696007811933 0.000702437410697 -6.42959876416e-06\n",
      "round 134 : 0.00107082090582 0.00107532153315 -4.50062733352e-06\n",
      "round 135 : 0.000867553833343 0.000875538177293 -7.98434395026e-06\n",
      "round 136 : 0.000944980933074 0.000952642847225 -7.66191415164e-06\n",
      "round 137 : 0.000887407244846 0.000889155493365 -1.74824851954e-06\n",
      "round 138 : 0.00073326062275 0.000735177370795 -1.91674804498e-06\n",
      "round 139 : 0.000882567064221 0.000889181187956 -6.61412373486e-06\n",
      "round 140 : 0.000780266387057 0.000783471035005 -3.20464794834e-06\n",
      "round 141 : 0.00107436126479 0.0010724369085 1.92435628708e-06\n",
      "round 142 : 0.000959843304671 0.000955677017108 4.16628756308e-06\n",
      "round 143 : 0.000732238889414 0.000733130949048 -8.92059634914e-07\n",
      "round 144 : 0.000920142086622 0.00092312542921 -2.98334258881e-06\n",
      "round 145 : 0.00127088413354 0.00126591565005 4.96848348548e-06\n",
      "round 146 : 0.000871561197598 0.000873566720708 -2.00552310989e-06\n",
      "round 147 : 0.000746414682032 0.000743944463493 2.4702185392e-06\n",
      "round 148 : 0.000978452039296 0.000983528439641 -5.07640034543e-06\n",
      "round 149 : 0.00094132894889 0.000942323009624 -9.94060734363e-07\n",
      "round 150 : 0.000994824552073 0.00100373129695 -8.90674488068e-06\n",
      "round 151 : 0.000950204986776 0.000954134756831 -3.92977005509e-06\n",
      "round 152 : 0.000904152171321 0.000899368490295 4.78368102556e-06\n",
      "round 153 : 0.000892824610493 0.000885631980138 7.19263035589e-06\n",
      "round 154 : 0.000998432069631 0.00100040669801 -1.97462837864e-06\n",
      "round 155 : 0.000903186519368 0.000899737340176 3.44917919209e-06\n",
      "round 156 : 0.00107192255871 0.00106592403344 5.99852526716e-06\n",
      "round 157 : 0.00095773929134 0.000960932555956 -3.19326461679e-06\n",
      "round 158 : 0.000900491698255 0.000902660007046 -2.16830879057e-06\n",
      "round 159 : 0.00113849989018 0.00113847940233 2.04878472868e-08\n",
      "round 160 : 0.00125975863477 0.0012469490169 1.28096178746e-05\n",
      "round 161 : 0.000893661568703 0.000894047541375 -3.85972671708e-07\n",
      "round 162 : 0.00100543923124 0.00101664141515 -1.12021839083e-05\n",
      "round 163 : 0.00109855042548 0.00110274349661 -4.19307112634e-06\n",
      "round 164 : 0.00109601999185 0.00110063709407 -4.61710221448e-06\n",
      "round 165 : 0.00105259633701 0.0010571541206 -4.55778358408e-06\n",
      "round 166 : 0.00067920349435 0.000678903540426 2.99953924689e-07\n",
      "round 167 : 0.000902995607643 0.000901540531776 1.45507586646e-06\n",
      "round 168 : 0.00105844062544 0.00105749545871 9.45166725883e-07\n",
      "round 169 : 0.000933082787771 0.000935188376851 -2.10558907951e-06\n",
      "round 170 : 0.000615407799935 0.000621735487514 -6.32768757869e-06\n",
      "round 171 : 0.000737147335854 0.000740860637045 -3.71330119073e-06\n",
      "round 172 : 0.000755350811893 0.000757477903052 -2.12709115898e-06\n",
      "round 173 : 0.000706033076708 0.000707317545769 -1.28446906101e-06\n",
      "round 174 : 0.000802163019024 0.000805254211639 -3.0911926153e-06\n",
      "round 175 : 0.000838496957574 0.000844734730936 -6.23777336202e-06\n",
      "round 176 : 0.00103885466859 0.00104320144831 -4.34677972247e-06\n",
      "round 177 : 0.000867564773143 0.000873149452578 -5.58467943553e-06\n",
      "round 178 : 0.000937283672946 0.000943898204706 -6.61453176036e-06\n",
      "round 179 : 0.000867444357056 0.00086886952513 -1.42516807403e-06\n",
      "round 180 : 0.00086795113673 0.00086538827842 2.56285831025e-06\n",
      "round 181 : 0.00079004468624 0.000783453613046 6.59107319392e-06\n",
      "round 182 : 0.000777582914833 0.000775827302164 1.75561266857e-06\n",
      "round 183 : 0.00111941494982 0.00111268551392 6.72943590325e-06\n",
      "round 184 : 0.000960827731335 0.000961129063104 -3.01331769656e-07\n",
      "round 185 : 0.00117981587081 0.00117977363296 4.22378521475e-08\n",
      "round 186 : 0.000951706773435 0.000951254286446 4.52486989518e-07\n",
      "round 187 : 0.00104772026485 0.00104831142439 -5.91159538045e-07\n",
      "round 188 : 0.000970699447387 0.000973339164821 -2.63971743388e-06\n",
      "round 189 : 0.00107262681036 0.00107377347988 -1.14666951397e-06\n",
      "round 190 : 0.000990828202557 0.000993559462728 -2.73126017131e-06\n",
      "round 191 : 0.00116795636227 0.00116446315482 3.49320745497e-06\n",
      "round 192 : 0.00111529398233 0.00110806272582 7.23125651242e-06\n",
      "round 193 : 0.000925718767137 0.000918538328689 7.18043844743e-06\n",
      "round 194 : 0.000899044612076 0.0008907603865 8.28422557561e-06\n",
      "round 195 : 0.00102202433869 0.00102000423 2.02010869117e-06\n",
      "round 196 : 0.000960081604205 0.000960386683303 -3.05079098786e-07\n",
      "round 197 : 0.000760276881247 0.000760155077845 1.21803401847e-07\n",
      "round 198 : 0.000813214992887 0.00081453914058 -1.32414769241e-06\n",
      "round 199 : 0.000776854100892 0.000779398848917 -2.54474802518e-06\n",
      "round 200 : 0.00113036853904 0.00113685467031 -6.48613127082e-06\n",
      "round 201 : 0.000838238150262 0.000840869594422 -2.63144416005e-06\n",
      "0.189507011526 0.189778254357\n"
     ]
    }
   ],
   "source": [
    "for (index, element) in enumerate(y_test):\n",
    "    print(\"round\", index, \":\", error_sgd[index], error_adam[index], error_sgd[index] - error_adam[index])\n",
    "    \n",
    "print(sum(error_sgd), sum(error_adam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
