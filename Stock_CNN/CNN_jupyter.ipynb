{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from statistics import mode\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, Dropout, Activation, Flatten, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SET50 (unrealize)\n",
    "SET50 = ['ADVANC', 'AOT', 'BANPU', 'BBL', 'BCP', 'BDMS',\n",
    "        'BEM', 'BH', 'BJC', 'BTS', 'CENTEL', 'CPALL', \n",
    "         'CPF', 'CPN', 'DTAC', 'EGCO', 'GLOBAL', 'HMPRO', \n",
    "         'INTUCH', 'IRPC', 'KBANK', 'KCE', 'KKP', 'KTB', 'LH', 'MINT', 'PTT',\n",
    "        'PTTEP', 'ROBINS', 'SCB', 'SCC', \n",
    "        'TCAP', 'TISCO', 'TMB', 'TOP', 'TRUE', 'TU']\n",
    "\n",
    "# remove (low data) : TPIPP WHA SPRC SAWAD PSH MTLS IVL GPSC EA CBG BPP BEAUTY PTTGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_stock_data(symbol, year_start, year_end):\n",
    "    stock_data = []\n",
    "    with open('./SET50/' + symbol + '.BK.csv', 'r') as csv_file:\n",
    "        file_data = csv.reader(csv_file, delimiter=',')\n",
    "        file_data = list(file_data)[1:]\n",
    "        temp_value = 0\n",
    "        for row in file_data:\n",
    "            if row[2] is '':\n",
    "                continue\n",
    "            elif int(row[0][0:4]) >= year_start and int(row[0][0:4]) <= year_end:\n",
    "                temp = (float(row[2]) + float(row[3]))/2.0\n",
    "                if temp_value != 0:\n",
    "                    unrealize = (temp - temp_value) / temp_value\n",
    "                else:\n",
    "                    unrealize = 0\n",
    "                unrealize = [\"{0:.4f}\".format(unrealize)]\n",
    "                stock_data.append(unrealize)\n",
    "                temp_value = temp\n",
    "                \n",
    "    return stock_data\n",
    "\n",
    "def mean_square_error(actual, predict):\n",
    "    length = len(actual)\n",
    "    sums_error = 0.0\n",
    "    for i, j in zip(actual, predict):\n",
    "        sums_error += (i - j)**2\n",
    "    \n",
    "    return sums_error / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findNewXTest(x_test, predict):\n",
    "    for (index, element) in enumerate(x_test[0]):\n",
    "        for i in range(len(element)):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            elif i < 29:\n",
    "                element[i - 1] = element[i]\n",
    "            else:\n",
    "                element[i - 1] = element[i]\n",
    "                element[i] = predict[index]\n",
    "    return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2043"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_datas = [ load_stock_data(x, 2007, 2017) for x in SET50 ]\n",
    "\n",
    "min_count = len(stock_datas[0])\n",
    "\n",
    "for index, element in enumerate(stock_datas):\n",
    "    if len(element) < min_count:\n",
    "        min_count = len(element)\n",
    "\n",
    "min_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(64, (3, 3), input_shape=(len(stock_datas), 30, 1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3 ,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))   \n",
    "      \n",
    "model.add(Flatten())  \n",
    "model.add(Dense(512))  \n",
    "model.add(Activation('relu'))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(stock_datas)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = SGD(lr = 0.01, decay= 1e-6, momentum = 0.9, nesterov = True)\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounds = min_count - 30\n",
    "train_index = int(rounds * 0.8)\n",
    "validate_index = train_index + int(rounds * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610, 37, 30, 1) (201, 37, 30, 1) (1, 37, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "count = 0\n",
    "index = 0\n",
    "while index < rounds:\n",
    "    temp = [i[index : index + 30] for i in stock_datas]\n",
    "    data.append(temp)\n",
    "    index += 1\n",
    "    \n",
    "x = np.asarray(data)\n",
    "x_train = x[0: train_index].astype('float32')\n",
    "x_validate = x[train_index: validate_index].astype('float32')\n",
    "x_test = x[validate_index: validate_index + 1].astype('float32')\n",
    "\n",
    "print(x_train.shape, x_validate.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1610, 37) (201, 37) (202, 37)\n"
     ]
    }
   ],
   "source": [
    "target = []\n",
    "index = 0\n",
    "while index < rounds:\n",
    "    temp = [stock_datas[i][index + 30][0] for i in range(len(stock_datas))]\n",
    "    target.append(temp)\n",
    "    index += 1\n",
    "    \n",
    "y = np.asarray(target)\n",
    "y_train = y[0: train_index].astype('float32')\n",
    "y_validate = y[train_index: validate_index].astype('float32')\n",
    "y_test = y[validate_index: rounds].astype('float32')\n",
    "\n",
    "print(y_train.shape, y_validate.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1610 samples, validate on 201 samples\n",
      "Epoch 1/100\n",
      "1610/1610 [==============================] - 1s 805us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 2/100\n",
      "1610/1610 [==============================] - 1s 496us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 3/100\n",
      "1610/1610 [==============================] - 1s 494us/step - loss: 0.0016 - acc: 0.0335 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 4/100\n",
      "1610/1610 [==============================] - 1s 508us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 5/100\n",
      "1610/1610 [==============================] - 1s 495us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 6/100\n",
      "1610/1610 [==============================] - 1s 499us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 7/100\n",
      "1610/1610 [==============================] - 1s 509us/step - loss: 0.0016 - acc: 0.0366 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 8/100\n",
      "1610/1610 [==============================] - 1s 501us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 9/100\n",
      "1610/1610 [==============================] - 1s 583us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 10/100\n",
      "1610/1610 [==============================] - 1s 670us/step - loss: 0.0016 - acc: 0.0354 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 11/100\n",
      "1610/1610 [==============================] - 1s 674us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 12/100\n",
      "1610/1610 [==============================] - 1s 687us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 13/100\n",
      "1610/1610 [==============================] - 1s 688us/step - loss: 0.0016 - acc: 0.0335 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 14/100\n",
      "1610/1610 [==============================] - 1s 670us/step - loss: 0.0016 - acc: 0.0335 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 15/100\n",
      "1610/1610 [==============================] - 1s 679us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 16/100\n",
      "1610/1610 [==============================] - 1s 679us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 17/100\n",
      "1610/1610 [==============================] - 1s 684us/step - loss: 0.0016 - acc: 0.0354 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 18/100\n",
      "1610/1610 [==============================] - 1s 661us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 19/100\n",
      "1610/1610 [==============================] - 1s 674us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 20/100\n",
      "1610/1610 [==============================] - 1s 668us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 21/100\n",
      "1610/1610 [==============================] - 1s 663us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 22/100\n",
      "1610/1610 [==============================] - 1s 679us/step - loss: 0.0016 - acc: 0.0360 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 23/100\n",
      "1610/1610 [==============================] - 1s 683us/step - loss: 0.0016 - acc: 0.0410 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 24/100\n",
      "1610/1610 [==============================] - 1s 698us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2413e-04 - val_acc: 0.0945\n",
      "Epoch 25/100\n",
      "1610/1610 [==============================] - 1s 687us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2413e-04 - val_acc: 0.0995\n",
      "Epoch 26/100\n",
      "1610/1610 [==============================] - 1s 688us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2413e-04 - val_acc: 0.0995\n",
      "Epoch 27/100\n",
      "1610/1610 [==============================] - 1s 686us/step - loss: 0.0016 - acc: 0.0342 - val_loss: 9.2413e-04 - val_acc: 0.0995\n",
      "Epoch 28/100\n",
      "1610/1610 [==============================] - 1s 678us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2413e-04 - val_acc: 0.0995\n",
      "Epoch 29/100\n",
      "1610/1610 [==============================] - 1s 683us/step - loss: 0.0016 - acc: 0.0360 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 30/100\n",
      "1610/1610 [==============================] - 1s 671us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 31/100\n",
      "1610/1610 [==============================] - 1s 668us/step - loss: 0.0016 - acc: 0.0323 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 32/100\n",
      "1610/1610 [==============================] - 1s 687us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 33/100\n",
      "1610/1610 [==============================] - 1s 683us/step - loss: 0.0016 - acc: 0.0366 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 34/100\n",
      "1610/1610 [==============================] - 1s 673us/step - loss: 0.0016 - acc: 0.0242 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 35/100\n",
      "1610/1610 [==============================] - 1s 673us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 36/100\n",
      "1610/1610 [==============================] - 1s 694us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 37/100\n",
      "1610/1610 [==============================] - 1s 701us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 38/100\n",
      "1610/1610 [==============================] - 1s 690us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 39/100\n",
      "1610/1610 [==============================] - 1s 729us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 40/100\n",
      "1610/1610 [==============================] - 1s 740us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 41/100\n",
      "1610/1610 [==============================] - 1s 716us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2413e-04 - val_acc: 0.1045\n",
      "Epoch 42/100\n",
      "1610/1610 [==============================] - 1s 720us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 43/100\n",
      "1610/1610 [==============================] - 1s 746us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 44/100\n",
      "1610/1610 [==============================] - 1s 715us/step - loss: 0.0016 - acc: 0.0199 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 45/100\n",
      "1610/1610 [==============================] - 1s 697us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 46/100\n",
      "1610/1610 [==============================] - 1s 693us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 47/100\n",
      "1610/1610 [==============================] - 1s 705us/step - loss: 0.0016 - acc: 0.0360 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 48/100\n",
      "1610/1610 [==============================] - 1s 730us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 49/100\n",
      "1610/1610 [==============================] - 1s 714us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 50/100\n",
      "1610/1610 [==============================] - 1s 683us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 51/100\n",
      "1610/1610 [==============================] - 1s 662us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 52/100\n",
      "1610/1610 [==============================] - 1s 539us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 53/100\n",
      "1610/1610 [==============================] - 1s 505us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 54/100\n",
      "1610/1610 [==============================] - 1s 496us/step - loss: 0.0016 - acc: 0.0366 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 55/100\n",
      "1610/1610 [==============================] - 1s 509us/step - loss: 0.0016 - acc: 0.0373 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 56/100\n",
      "1610/1610 [==============================] - 1s 502us/step - loss: 0.0016 - acc: 0.0261 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 57/100\n",
      "1610/1610 [==============================] - 1s 500us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 58/100\n",
      "1610/1610 [==============================] - 1s 504us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 59/100\n",
      "1610/1610 [==============================] - 1s 504us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 60/100\n",
      "1610/1610 [==============================] - 1s 528us/step - loss: 0.0016 - acc: 0.0329 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 61/100\n",
      "1610/1610 [==============================] - 1s 498us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 62/100\n",
      "1610/1610 [==============================] - 1s 498us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 63/100\n",
      "1610/1610 [==============================] - 1s 500us/step - loss: 0.0016 - acc: 0.0354 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 64/100\n",
      "1610/1610 [==============================] - 1s 508us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 65/100\n",
      "1610/1610 [==============================] - 1s 503us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 66/100\n",
      "1610/1610 [==============================] - 1s 503us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 67/100\n",
      "1610/1610 [==============================] - 1s 506us/step - loss: 0.0016 - acc: 0.0230 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 68/100\n",
      "1610/1610 [==============================] - 1s 512us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 69/100\n",
      "1610/1610 [==============================] - 1s 495us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 70/100\n",
      "1610/1610 [==============================] - 1s 521us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 71/100\n",
      "1610/1610 [==============================] - 1s 499us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 72/100\n",
      "1610/1610 [==============================] - 1s 497us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 73/100\n",
      "1610/1610 [==============================] - 1s 498us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 74/100\n",
      "1610/1610 [==============================] - 1s 516us/step - loss: 0.0016 - acc: 0.0366 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 75/100\n",
      "1610/1610 [==============================] - 1s 506us/step - loss: 0.0016 - acc: 0.0323 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 76/100\n",
      "1610/1610 [==============================] - 1s 500us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 77/100\n",
      "1610/1610 [==============================] - 1s 512us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 78/100\n",
      "1610/1610 [==============================] - 1s 510us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 79/100\n",
      "1610/1610 [==============================] - 1s 510us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 80/100\n",
      "1610/1610 [==============================] - 1s 510us/step - loss: 0.0016 - acc: 0.0317 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 81/100\n",
      "1610/1610 [==============================] - 1s 502us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 82/100\n",
      "1610/1610 [==============================] - 1s 501us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 83/100\n",
      "1610/1610 [==============================] - 1s 514us/step - loss: 0.0016 - acc: 0.0236 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 84/100\n",
      "1610/1610 [==============================] - 1s 503us/step - loss: 0.0016 - acc: 0.0248 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 85/100\n",
      "1610/1610 [==============================] - 1s 500us/step - loss: 0.0016 - acc: 0.0311 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 86/100\n",
      "1610/1610 [==============================] - 1s 503us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 87/100\n",
      "1610/1610 [==============================] - 1s 494us/step - loss: 0.0016 - acc: 0.0329 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 88/100\n",
      "1610/1610 [==============================] - 1s 510us/step - loss: 0.0016 - acc: 0.0292 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 89/100\n",
      "1610/1610 [==============================] - 1s 495us/step - loss: 0.0016 - acc: 0.0416 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 90/100\n",
      "1610/1610 [==============================] - 1s 494us/step - loss: 0.0016 - acc: 0.0255 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 91/100\n",
      "1610/1610 [==============================] - 1s 515us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 92/100\n",
      "1610/1610 [==============================] - 1s 510us/step - loss: 0.0016 - acc: 0.0205 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 93/100\n",
      "1610/1610 [==============================] - 1s 481us/step - loss: 0.0016 - acc: 0.0273 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 94/100\n",
      "1610/1610 [==============================] - 1s 521us/step - loss: 0.0016 - acc: 0.0329 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 95/100\n",
      "1610/1610 [==============================] - 1s 495us/step - loss: 0.0016 - acc: 0.0304 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 96/100\n",
      "1610/1610 [==============================] - 1s 524us/step - loss: 0.0016 - acc: 0.0267 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 97/100\n",
      "1610/1610 [==============================] - 1s 519us/step - loss: 0.0016 - acc: 0.0348 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 98/100\n",
      "1610/1610 [==============================] - 1s 498us/step - loss: 0.0016 - acc: 0.0298 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 99/100\n",
      "1610/1610 [==============================] - 1s 491us/step - loss: 0.0016 - acc: 0.0286 - val_loss: 9.2412e-04 - val_acc: 0.1045\n",
      "Epoch 100/100\n",
      "1610/1610 [==============================] - 1s 488us/step - loss: 0.0016 - acc: 0.0280 - val_loss: 9.2412e-04 - val_acc: 0.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ce6c87d710>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "model.fit(x_train, y_train, validation_data = (x_validate, y_validate), epochs = epochs, shuffle = True, batch_size = 100, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rounds: 1 0.000665932152525\n",
      "rounds: 2 0.000818042309741\n",
      "rounds: 3 0.000707070523194\n",
      "rounds: 4 0.000815524890859\n",
      "rounds: 5 0.000696879246718\n",
      "rounds: 6 0.00094520740228\n",
      "rounds: 7 0.00100218357256\n",
      "rounds: 8 0.000912143427204\n",
      "rounds: 9 0.00128069036648\n",
      "rounds: 10 0.000917052001024\n",
      "rounds: 11 0.000772102954704\n",
      "rounds: 12 0.00135703828994\n",
      "rounds: 13 0.00111123223562\n",
      "rounds: 14 0.00131835177689\n",
      "rounds: 15 0.00106144493362\n",
      "rounds: 16 0.000744946740437\n",
      "rounds: 17 0.00116519498126\n",
      "rounds: 18 0.000566749136464\n",
      "rounds: 19 0.000720133273919\n",
      "rounds: 20 0.000744760466603\n",
      "rounds: 21 0.000740209485657\n",
      "rounds: 22 0.00102340117713\n",
      "rounds: 23 0.000579099127272\n",
      "rounds: 24 0.000639306789878\n",
      "rounds: 25 0.000844651320683\n",
      "rounds: 26 0.00069323715424\n",
      "rounds: 27 0.000707821405678\n",
      "rounds: 28 0.000575535803083\n",
      "rounds: 29 0.000803533206787\n",
      "rounds: 30 0.000783245323337\n",
      "rounds: 31 0.000843495276404\n",
      "rounds: 32 0.00090359900804\n",
      "rounds: 33 0.000626948000165\n",
      "rounds: 34 0.000783757551465\n",
      "rounds: 35 0.00081514500482\n",
      "rounds: 36 0.00055095605893\n",
      "rounds: 37 0.000889682031259\n",
      "rounds: 38 0.000575395214069\n",
      "rounds: 39 0.000804480599654\n",
      "rounds: 40 0.00107978798814\n",
      "rounds: 41 0.000978291415765\n",
      "rounds: 42 0.000999236594007\n",
      "rounds: 43 0.00125854812211\n",
      "rounds: 44 0.00098618110566\n",
      "rounds: 45 0.00101593908823\n",
      "rounds: 46 0.000870155683184\n",
      "rounds: 47 0.000738583162725\n",
      "rounds: 48 0.000871024565349\n",
      "rounds: 49 0.000740621845085\n",
      "rounds: 50 0.000689394828412\n",
      "rounds: 51 0.000680850205173\n",
      "rounds: 52 0.00073799999827\n",
      "rounds: 53 0.000835609663629\n",
      "rounds: 54 0.00109024784376\n",
      "rounds: 55 0.000980988236712\n",
      "rounds: 56 0.00128464265673\n",
      "rounds: 57 0.0011328159712\n",
      "rounds: 58 0.00101496621221\n",
      "rounds: 59 0.0014807756164\n",
      "rounds: 60 0.00112029218697\n",
      "rounds: 61 0.000923079881021\n",
      "rounds: 62 0.000947422287311\n",
      "rounds: 63 0.000819801439689\n",
      "rounds: 64 0.000660634963245\n",
      "rounds: 65 0.00100717047837\n",
      "rounds: 66 0.00148012379332\n",
      "rounds: 67 0.000999573542318\n",
      "rounds: 68 0.000704854895435\n",
      "rounds: 69 0.00118814937773\n",
      "rounds: 70 0.000857224063775\n",
      "rounds: 71 0.000666437494124\n",
      "rounds: 72 0.00106084336366\n",
      "rounds: 73 0.000800358174127\n",
      "rounds: 74 0.000651478485968\n",
      "rounds: 75 0.000685125847167\n",
      "rounds: 76 0.000664859310427\n",
      "rounds: 77 0.00057302609892\n",
      "rounds: 78 0.000870424134451\n",
      "rounds: 79 0.000867374306467\n",
      "rounds: 80 0.000934693124236\n",
      "rounds: 81 0.00125134805419\n",
      "rounds: 82 0.000874284916286\n",
      "rounds: 83 0.00124632496447\n",
      "rounds: 84 0.00116101288758\n",
      "rounds: 85 0.000700781314706\n",
      "rounds: 86 0.000639776887534\n",
      "rounds: 87 0.000808239738297\n",
      "rounds: 88 0.000773583300821\n",
      "rounds: 89 0.000901207672035\n",
      "rounds: 90 0.00102979471703\n",
      "rounds: 91 0.000851138461366\n",
      "rounds: 92 0.000945150132507\n",
      "rounds: 93 0.00093607906393\n",
      "rounds: 94 0.000999931914236\n",
      "rounds: 95 0.0011903261673\n",
      "rounds: 96 0.00170617234749\n",
      "rounds: 97 0.00160673691534\n",
      "rounds: 98 0.0018155196771\n",
      "rounds: 99 0.00175092517939\n",
      "rounds: 100 0.0021522804628\n",
      "rounds: 101 0.000969206868215\n",
      "rounds: 102 0.000665549860796\n",
      "rounds: 103 0.00157391202521\n",
      "rounds: 104 0.000752573974598\n",
      "rounds: 105 0.00135746391017\n",
      "rounds: 106 0.00132911258379\n",
      "rounds: 107 0.00105252758303\n",
      "rounds: 108 0.00217697891967\n",
      "rounds: 109 0.00075925866799\n",
      "rounds: 110 0.000763713245757\n",
      "rounds: 111 0.00105084114994\n",
      "rounds: 112 0.000971451064834\n",
      "rounds: 113 0.00110577854643\n",
      "rounds: 114 0.000594659467267\n",
      "rounds: 115 0.00068357360287\n",
      "rounds: 116 0.00106664238521\n",
      "rounds: 117 0.00107862193932\n",
      "rounds: 118 0.00109030840542\n",
      "rounds: 119 0.000926311403447\n",
      "rounds: 120 0.000719382506122\n",
      "rounds: 121 0.000905751976951\n",
      "rounds: 122 0.000652349613847\n",
      "rounds: 123 0.000617408802192\n",
      "rounds: 124 0.000623167126323\n",
      "rounds: 125 0.000642468560024\n",
      "rounds: 126 0.000678909996693\n",
      "rounds: 127 0.0009222475978\n",
      "rounds: 128 0.00118795629889\n",
      "rounds: 129 0.00103118062088\n",
      "rounds: 130 0.00101118422914\n",
      "rounds: 131 0.00096099382326\n",
      "rounds: 132 0.000711987088374\n",
      "rounds: 133 0.000723973431108\n",
      "rounds: 134 0.000695506792254\n",
      "rounds: 135 0.00107099533572\n",
      "rounds: 136 0.000867500180155\n",
      "rounds: 137 0.000944277634669\n",
      "rounds: 138 0.000887000473459\n",
      "rounds: 139 0.000733153053647\n",
      "rounds: 140 0.000882276954627\n",
      "rounds: 141 0.000780597867614\n",
      "rounds: 142 0.00107459748289\n",
      "rounds: 143 0.000959795452571\n",
      "rounds: 144 0.000732204074642\n",
      "rounds: 145 0.000919704781932\n",
      "rounds: 146 0.00127094529389\n",
      "rounds: 147 0.000871840808496\n",
      "rounds: 148 0.000746712603785\n",
      "rounds: 149 0.000978017039599\n",
      "rounds: 150 0.000941115283106\n",
      "rounds: 151 0.00099399830214\n",
      "rounds: 152 0.000949705142619\n",
      "rounds: 153 0.000904053869228\n",
      "rounds: 154 0.000893392254884\n",
      "rounds: 155 0.000999282237673\n",
      "rounds: 156 0.000903596361735\n",
      "rounds: 157 0.0010726603916\n",
      "rounds: 158 0.000957194079097\n",
      "rounds: 159 0.00090009458689\n",
      "rounds: 160 0.00113804273307\n",
      "rounds: 161 0.00125970502479\n",
      "rounds: 162 0.000893585420181\n",
      "rounds: 163 0.00100616182767\n",
      "rounds: 164 0.00109895850773\n",
      "rounds: 165 0.00109641964865\n",
      "rounds: 166 0.00105212359628\n",
      "rounds: 167 0.00067933367783\n",
      "rounds: 168 0.000903035290232\n",
      "rounds: 169 0.00105775527627\n",
      "rounds: 170 0.000932696075745\n",
      "rounds: 171 0.000614997214601\n",
      "rounds: 172 0.000736983546884\n",
      "rounds: 173 0.000755538480987\n",
      "rounds: 174 0.000706175541085\n",
      "rounds: 175 0.000801842694982\n",
      "rounds: 176 0.000837570922256\n",
      "rounds: 177 0.00103740910839\n",
      "rounds: 178 0.000867202205676\n",
      "rounds: 179 0.00093781970045\n",
      "rounds: 180 0.000868330132139\n",
      "rounds: 181 0.000868585867551\n",
      "rounds: 182 0.000790781786122\n",
      "rounds: 183 0.000777718021923\n",
      "rounds: 184 0.00111937197508\n",
      "rounds: 185 0.00096081972962\n",
      "rounds: 186 0.00117996186285\n",
      "rounds: 187 0.000951749337741\n",
      "rounds: 188 0.00104756956216\n",
      "rounds: 189 0.000971394846552\n",
      "rounds: 190 0.00107289561366\n",
      "rounds: 191 0.000991154844825\n",
      "rounds: 192 0.00116815171564\n",
      "rounds: 193 0.00111575016587\n",
      "rounds: 194 0.000926543483968\n",
      "rounds: 195 0.000898850257555\n",
      "rounds: 196 0.00102187638739\n",
      "rounds: 197 0.000959897816141\n",
      "rounds: 198 0.000759915228466\n",
      "rounds: 199 0.000813348332877\n",
      "rounds: 200 0.00077699366645\n",
      "rounds: 201 0.0011311459233\n",
      "rounds: 202 0.000837909814796\n"
     ]
    }
   ],
   "source": [
    "for (index, element) in enumerate(y_test):\n",
    "    predict = model.predict(x_test, verbose = 0)\n",
    "    error = mean_square_error(element, predict[0])\n",
    "    print(\"rounds:\",index + 1, error)\n",
    "    x_test = findNewXTest(x_test, predict[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
